{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef94a7d4-471c-41cd-ac41-909cbd199cdc",
   "metadata": {},
   "source": [
    "# Survey non-uniformity check pipeline:\n",
    "\n",
    "This is the draft RAIL pipeline for checking, given choice of photo-z pipeline, the effect of survey non-uniformity.\n",
    "\n",
    "for this also c.f. `golden spike rail_pipelines` under the DESC directory making a pipeline class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad4feb6-fd8d-4668-b971-64098b3b123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.io import fits\n",
    "import healpy as hp\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import yaml\n",
    "\n",
    "import matplotlib\n",
    "cmap = matplotlib.cm.get_cmap('plasma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e348efa-f287-4402-99ae-b95bfcb1e1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAIL modules:\n",
    "from rail.core.data import TableHandle\n",
    "from rail.core.stage import RailStage\n",
    "\n",
    "DS = RailStage.data_store\n",
    "DS.__class__.allow_overwrite = True\n",
    "\n",
    "#import pzflow\n",
    "#from pzflow import Flow\n",
    "from rail.creation.engines.flowEngine import FlowCreator\n",
    "\n",
    "from rail.creation.degradation import observing_condition_degrader\n",
    "from rail.creation.degradation.observing_condition_degrader import ObsCondition\n",
    "\n",
    "\n",
    "import tables_io\n",
    "\n",
    "from rail.estimation.algos.flexzboost import Inform_FZBoost, FZBoost\n",
    "from rail.estimation.algos.bpz_lite import BPZ_lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05b50fd-66f1-41c1-9f56-34b9b9e96631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dust (de-)reddening:\n",
    "import dustmaps\n",
    "from dustmaps.sfd import SFDQuery\n",
    "from astropy.coordinates import SkyCoord\n",
    "from dustmaps.config import config\n",
    "config['data_dir'] = '/global/cfs/cdirs/lsst/groups/PZ/PhotoZDC2/run2.2i_dr6_test/TESTDUST/mapdata' \n",
    "# above path may be / have been updated..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81423f10-357b-41c7-878b-1889fc3f6f6e",
   "metadata": {},
   "source": [
    "User defined functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcdbc95-3cb7-4020-9f0e-2153273e06ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 0.5: append semi-major/minor axis of the galaxy from size and ellipticity\n",
    "\n",
    "def get_semi_major_minor(catalog, scale=1):\n",
    "\n",
    "    q = (1 - catalog['ellipticity'])/(1 + catalog['ellipticity'])\n",
    "    ai = catalog['size']\n",
    "    bi = ai*q\n",
    "\n",
    "    ai = ai.to_numpy()*scale\n",
    "    bi = bi.to_numpy()*scale\n",
    "    \n",
    "    d = {\"major\": ai, \"minor\": bi}\n",
    "    major_minor_axis = pd.DataFrame(data=d)\n",
    "    \n",
    "    # append to the input data\n",
    "    catalog = pd.concat([catalog, major_minor_axis], axis=1)\n",
    "    \n",
    "    return catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803e3df7-14fc-420e-bf57-d9bb78e97603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deredden_galaxy(data, bands, nside=128):\n",
    "    \n",
    "    # set the A_lamba/E(B-V) values for the six LSST filters \n",
    "    band_a_ebv = np.array([4.81,3.64,2.70,2.06,1.58,1.31])\n",
    "    \n",
    "    # turn assigned pixels to ra, dec:\n",
    "    assigned_pixels = data[\"pixels\"]\n",
    "    ra, dec = hp.pix2ang(nside, assigned_pixels, lonlat=True)\n",
    "    coords = SkyCoord(ra, dec, unit = 'deg', frame='fk5')\n",
    "    # compute EBV\n",
    "    sfd = SFDQuery()\n",
    "    ebvvec = sfd(coords)\n",
    "\n",
    "    mag_dered = pd.DataFrame()\n",
    "    for ii, b in enumerate(bands):\n",
    "        mag_dered[b] = data[b]-ebvvec*band_a_ebv[ii]\n",
    "    \n",
    "    return mag_dered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4a678c-cbf7-44d9-8cb4-bf40f3f7f68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_catalog_to_test_data(data, DS, bands, nside=128, dered=False):\n",
    "\n",
    "    data2 = OrderedDict()\n",
    "    \n",
    "    if dered == True:\n",
    "        mag_dered = deredden_galaxy(data, bands, nside=nside)\n",
    "    \n",
    "    for bb in bands:\n",
    "        data2['%s_err'%bb] = data['%s_err'%bb].to_numpy()\n",
    "        if dered == False:\n",
    "            data2[bb] = data[bb].to_numpy()\n",
    "        elif dered == True:\n",
    "            data2[bb] = mag_dered[bb].to_numpy()\n",
    "        \n",
    "    data2['redshift'] = data['redshift'].to_numpy()\n",
    "\n",
    "    xtest_data = tables_io.convert(data2, tables_io.types.NUMPY_DICT)\n",
    "    test_data = DS.add_data(\"test_data\", xtest_data, TableHandle)\n",
    "    \n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcde33e-8317-4a80-87a4-3172a14463ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the simplist binning method using the pz point estimate directly:\n",
    "\n",
    "def assign_lens_bins(catalog, nYrObs=1):\n",
    "    \"\"\"\n",
    "    nYrObs: lens binning requirement to adopt.\n",
    "    \"\"\"\n",
    "    pz = catalog['pz_point'].to_numpy()\n",
    "    if nYrObs==1:\n",
    "        # adopt 1 year criteria: 5 bins between 0.2<z<1.2\n",
    "        bin_edges = np.linspace(0.2,1.2,6)\n",
    "        bin_index = np.digitize(pz, bin_edges) \n",
    "    if nYrObs==10:\n",
    "        # adopt 10 year criteria: 10 bins between 0.2<z<1.2\n",
    "        bin_edges = np.linspace(0.2,1.2,11)\n",
    "        bin_index = np.digitize(pz, bin_edges) \n",
    "\n",
    "    # append the tomographic binning\n",
    "    tomo = pd.DataFrame(data={\"tomo\": bin_index})\n",
    "    catalog_tomo = pd.concat([catalog, tomo], axis=1)\n",
    "    # trim the catalogue for objects not in the bin:\n",
    "    sel = np.where((bin_index>=1)&(bin_index<len(bin_edges)))[0]\n",
    "    catalog_tomo = catalog_tomo[sel]\n",
    "    \n",
    "    return catalog_tomo\n",
    "\n",
    "\n",
    "def assign_source_bins(catalog):\n",
    "    # requirement is 5 bins with equal number of objects:\n",
    "    # here we just use simple quantile from the estimated photo-z\n",
    "    pz = catalog['pz_point'].to_numpy()\n",
    "    nbins=5\n",
    "    sortind = np.sortarg(pz)\n",
    "    N_perbin = int(len(pz)/nbins)\n",
    "    bin_index = np.zeros(len(pz))\n",
    "    for ii in range(nbins):\n",
    "        useind = sortind[ii*N_perbin:(ii+1)*N_perbin]\n",
    "        bin_index[useind] = ii+1\n",
    "    \n",
    "    # append the tomographic binning\n",
    "    tomo = pd.DataFrame(data={\"tomo\": bin_index})\n",
    "    catalog_tomo = pd.concat([catalog, tomo], axis=1)\n",
    "    return catalog_tomo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb287941-a2b6-4560-841c-3709a6825c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sys_map_quantiles(mapin, mask, nquantiles=10):\n",
    "    \"\"\"\n",
    "    mapin: systematic map input\n",
    "    mask: mask for the systematic map\n",
    "    nquantiles: number of quantiles to split into, default is 10\n",
    "    \"\"\"\n",
    "    \n",
    "    pix = np.arange(len(mapin))\n",
    "    usemap = mapin[mask.astype(bool)]\n",
    "    usepix = pix[mask.astype(bool)]\n",
    "\n",
    "    sortind = np.argsort(usemap)\n",
    "    Npix_perq = int(len(usemap)/nquantiles)\n",
    "    \n",
    "    quantile=[]\n",
    "    meanv = np.zeros(len(nquantiles))\n",
    "    \n",
    "    for ii in range(nquantiles):\n",
    "        useind = sortind[ii*Npix_perq:(ii+1)*Npix_perq]\n",
    "        pix = usepix[useind]\n",
    "        #temp = np.zeros(len(mask))\n",
    "        #temp[pix]=1\n",
    "        quantile.append(pix)\n",
    "        # compute the mean value of the quantile:\n",
    "        meanv[ii] = np.mean(mapin[pix])\n",
    "    return quantile, meanv\n",
    "\n",
    "\n",
    "def compute_nzstats(usecat, z_col, zgrid=np.linspace(0,1,31), nbootstrap=None):\n",
    "    \"\"\"\n",
    "    Computes some summary statistics of the n_z for input catalog, specifically:\n",
    "    - nz: given zgrid, output will be [zcentre, Nz]\n",
    "    - meanz: given zgrid, weighted mean redshift with bootstrap error [meanz, meanz_err]\n",
    "    - sigmaz: given zgrid, weighted first moment of z (sigmaz) with bootstrap error [sigmaz, sigmaz_err]\n",
    "    \"\"\"\n",
    "    \n",
    "    Nz = np.histogram(usecat[z_col], bins=zgrid)\n",
    "    zcentre = (zgrid[1:] + zgrid[:-1])*0.5\n",
    "    nz = np.c_[zcentre, Nz]\n",
    "    \n",
    "    meanz = np.sum(Nz*zcentre)/np.sum(Nz)\n",
    "    \n",
    "    sigmaz = np.sqrt(np.sum(Nz*(zcentre-meanz)**2)/np.sum(Nz))\n",
    "    \n",
    "    # compute the bootstrap error:\n",
    "    sampholder_meanz = np.zeros(nbootstrap)\n",
    "    sampholder_sigmaz = np.zeros(nbootstrap)\n",
    "    \n",
    "    for kk in range(nbootstrap):\n",
    "        samp = np.random.choice(usecat[z_col], \n",
    "                        size=len(usecat[z_col]),\n",
    "                        replace=True)\n",
    "        # repeat the operation \n",
    "        cc = np.histogram(samp,bins=zgrid)\n",
    "        sampholder_meanz[kk] = np.sum(cc[0] * zcentre)/np.sum(cc[0])\n",
    "        sampholder_sigmaz[kk] = np.sqrt(np.sum(cc[0]*(zcentre-meanz)**2)/np.sum(cc[0]))\n",
    "        \n",
    "    meanz_err = np.std(sampholder_meanz)\n",
    "    sigmaz_err = np.std(sampholder_sigmaz)\n",
    "    \n",
    "    meanz = np.array([meanz, meanz_err])\n",
    "    sigmaz = np.array([sigmaz, sigmaz_err])\n",
    "    \n",
    "    return nz, meanz, sigmaz\n",
    "\n",
    "def write_evaluation_results(outroot, meanv, nzstat_summary_split, nzstat_summary_tot):\n",
    "    \n",
    "    ntomo = list(nzstat_summary_split.keys())\n",
    "    \n",
    "    out = {}\n",
    "    \n",
    "    out[\"nquantile\"] = len(meanv)\n",
    "    out[\"mean_systematic\"] = meanv\n",
    "    \n",
    "    for jj in ntomo:\n",
    "        out[\"tomo-%d\"%(jj+1)]={\n",
    "            \"nz\": [],\n",
    "            \"meanz\": np.array((len(meanv),2)),\n",
    "            \"sigmaz\": np.array((len(meanv),2)),\n",
    "        }\n",
    "        for kk in range(len(meanv)):\n",
    "            nz, meanz, sigmaz = nzstat_summary_split[\"tomo-%d\"%(jj+1)][kk]\n",
    "            out[\"tomo-%d\"%(jj+1)][\"nz\"].append(nz) \n",
    "            out[\"tomo-%d\"%(jj+1)][\"meanz\"][kk,:] = meanz\n",
    "            out[\"tomo-%d\"%(jj+1)][\"sigmaz\"][kk,:] = sigmaz\n",
    "            \n",
    "        # add unbinned stats:\n",
    "        nz, meanz, sigmaz = nzstat_summary_tot[\"tomo-%d\"%(jj+1)]\n",
    "        out[\"tomo-%d\"%(jj+1)][\"nztot\"]=nz \n",
    "        out[\"tomo-%d\"%(jj+1)][\"meanztot\"] = meanz\n",
    "        out[\"tomo-%d\"%(jj+1)][\"sigmaztot\"] = sigmaz\n",
    "            \n",
    "    # save to yaml file\n",
    "    file=open(outroot,\"w\")\n",
    "    yaml.dump(out,file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f68892-3283-4152-8987-0d33e15e9a2a",
   "metadata": {},
   "source": [
    "## Pre-defined variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f359992-c4fb-4094-9edc-6806cc95e40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained flows stored in here\n",
    "photFlow_dir = 'main_galaxy_flow/flow.pzflow.pkl'\n",
    "shapeFlow_dir = 'conditional_galaxy_flow/flow.pzflow.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8068ecd-afd4-42f4-9acc-ad7dfb81b5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first define a set of input map directories:\n",
    "\n",
    "base_path = \"/pscratch/sd/q/qhang/rubin_baseline_v2/MAF-1year/\"\n",
    "\n",
    "# nside of these maps:\n",
    "nside=128\n",
    "\n",
    "# seeing maps:\n",
    "seeing_u = base_path + \"baseline_v2_0_10yrs_Median_seeingFwhmEff_u_and_nightlt365_HEAL.fits\"\n",
    "seeing_g = base_path + \"baseline_v2_0_10yrs_Median_seeingFwhmEff_g_and_nightlt365_HEAL.fits\"\n",
    "seeing_r = base_path + \"baseline_v2_0_10yrs_Median_seeingFwhmEff_r_and_nightlt365_HEAL.fits\"\n",
    "seeing_i = base_path + \"baseline_v2_0_10yrs_Median_seeingFwhmEff_i_and_nightlt365_HEAL.fits\"\n",
    "seeing_z = base_path + \"baseline_v2_0_10yrs_Median_seeingFwhmEff_z_and_nightlt365_HEAL.fits\"\n",
    "seeing_y = base_path + \"baseline_v2_0_10yrs_Median_seeingFwhmEff_y_and_nightlt365_HEAL.fits\"\n",
    "\n",
    "# coadd depth maps:\n",
    "coaddm5_u = base_path + \"baseline_v2_0_10yrs_CoaddM5_u_and_nightlt365_HEAL.fits\"\n",
    "coaddm5_g = base_path + \"baseline_v2_0_10yrs_CoaddM5_g_and_nightlt365_HEAL.fits\"\n",
    "coaddm5_r = base_path + \"baseline_v2_0_10yrs_CoaddM5_r_and_nightlt365_HEAL.fits\"\n",
    "coaddm5_i = base_path + \"baseline_v2_0_10yrs_CoaddM5_i_and_nightlt365_HEAL.fits\"\n",
    "coaddm5_z = base_path + \"baseline_v2_0_10yrs_CoaddM5_z_and_nightlt365_HEAL.fits\"\n",
    "coaddm5_y = base_path + \"baseline_v2_0_10yrs_CoaddM5_y_and_nightlt365_HEAL.fits\"\n",
    "\n",
    "# here we will set the observing year and number of visits per year to 1, because we are supplying coadd depth\n",
    "\n",
    "# mask:\n",
    "maskdir = base_path + \"../wfd_footprint_nvisitcut_500_nside_128.fits\"\n",
    "\n",
    "# weight: for now we supply uniform weight\n",
    "\n",
    "# choose the systematic map to examine, here we choose the combined depth:\n",
    "sys_to_check = base_path + \"baseline_v2_0_10yrs_CoaddM5_i_and_nightlt365_HEAL.fits\"\n",
    "sys = \"CoaddM5\"\n",
    "\n",
    "# directory to save all the data:\n",
    "savedir = \"/pscratch/sd/q/qhang/PZflow-samples/DC2-test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb11f1b8-15ce-440c-853f-1e9fdda58fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibration of the error model \n",
    "# magerrscale currently not implemented, \n",
    "# probably need to modify the pipeline or save a new set of sys maps\n",
    "\n",
    "lsstError_calibration={\n",
    "    'semi_major_minor_scale':1/2.5,\n",
    "    'magerrscale':{\n",
    "        'u': 0.73,\n",
    "        'g': 1.20,\n",
    "        'r': 0.98, \n",
    "        'i': 1.10,\n",
    "        'z': 1.10,\n",
    "        'y': 1.15,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc95aa2-62a5-47cd-8f3a-0ec2b0eb67aa",
   "metadata": {},
   "source": [
    "## Step 1: RAIL.creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ecd7b0-e6df-46c2-a3fd-94cbaef829b6",
   "metadata": {},
   "source": [
    "### 1.1 Creation:\n",
    "In this example, we load pre-trained flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322d6e21-8354-4664-b08f-6175b57f40e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pzflow\n",
    "# below need testing to see if it works (original code works on pzflow)\n",
    "n_samples = 100000 #1e5 galaxies\n",
    "photFlow = FlowCreator.make_stage(name='photFlow', model=photFlow_dir, n_samples=n_samples)\n",
    "shapeFlow = FlowCreator.make_stage(name='shapeFlow', model=shapeFlow_dir, n_samples=n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3149643a-0783-4d76-ac3c-7b8a0b6c2aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the model is read in correctly:\n",
    "photFlow.get_data('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0e7ec1-116d-4ee9-825d-bf84f17d09e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define number of galaxies to generate:\n",
    "# still need to check if this line works...(original code works on pzflow)\n",
    "photoCat = photFlow.sample(n_samples, seed=0)\n",
    "fullCat = shapeFlow.sample(conditions=photoCat, seed=0)\n",
    "\n",
    "print(fullCat())\n",
    "print(\"Data was written to \", fullCat.path)\n",
    "# the fullCat should now be stored in the data Handle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e050b0-0714-4976-a32c-cb811ced16e5",
   "metadata": {},
   "source": [
    "### 1.2 Degradation:\n",
    "Use the degradation to specify a set of systematic maps, and generate observed magnitudes and magnitude errors:\n",
    "- Generate magnitude error according to survey condition maps supplied, generate degraded mag + magerr\n",
    "    - Note: galactic extinction to the cosmoDC2 magnitudes are not applied yet, probably need pipeline expansion to do it\n",
    "    - Note: currently `magerrscale` cannot be applied\n",
    "    - Note: can also specify the detection limit via `sigLim` (set to 3 sigma here);\n",
    "- Select data with i-band detection and flag non-detection in other bands as `np.nan`, apply further cuts such as the gold cut $i<23.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7eb9c2-7b35-4d1e-9e40-7b6d3efb1d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another option for including the magerrscale correction: \n",
    "# Adjust the m5 maps by:\n",
    "# m5 = m5 - 2.5*np.log10(lsstError_calibration['magerrscale'][band])\n",
    "# but currently we read maps via input file name\n",
    "# so maybe we should modify obs_cond to directly take arrays (maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac01f4b-b863-420e-b056-bf7d07964d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the degrader\n",
    "# list of arguement here\n",
    "# Note: number of years and visits set to 1 because coadd depth are supplied for m5\n",
    "y1_degrader = ObsCondition.make_stage(\n",
    "    map_dict={\n",
    "        \"theta\": {\n",
    "            \"u\": seeing_u,\n",
    "            \"g\": seeing_g,\n",
    "            \"r\": seeing_r,\n",
    "            \"i\": seeing_i,\n",
    "            \"z\": seeing_z,\n",
    "            \"y\": seeing_y,\n",
    "        },\n",
    "        \"m5\": {\n",
    "            \"u\": coaddm5_u,\n",
    "            \"g\": coaddm5_g,\n",
    "            \"r\": coaddm5_r,\n",
    "            \"i\": coaddm5_i,\n",
    "            \"z\": coaddm5_z,\n",
    "            \"y\": coaddm5_y,\n",
    "        },\n",
    "        \"nYrObs\": 1.,\n",
    "        \"nVisYr\": {\n",
    "            \"u\": 1.,\n",
    "            \"g\": 1.,\n",
    "            \"r\": 1.,\n",
    "            \"i\": 1.,\n",
    "            \"z\": 1.,\n",
    "            \"y\": 1., \n",
    "        },\n",
    "        \"sigLim\": 3,\n",
    "        \"ndFlag\": np.nan,\n",
    "        \"extendedType\": \"auto\",\n",
    "        \"majorCol\": \"major\",\n",
    "        \"minorCol\": \"minor\",\n",
    "        \"decorrelate\": True,\n",
    "        \"highSNR\": False,\n",
    "    },\n",
    "    nside=nside,\n",
    "    mask = maskdir,\n",
    "    weight = \"\",\n",
    ")\n",
    "\n",
    "# Compute the semi major and minor axes\n",
    "fullCat = get_semi_major_minor(catalog, scale=lsstError_calibration[\"semi_major_minor_scale\"])\n",
    "\n",
    "# degraded data below\n",
    "data_degraded = y1_degrader(fullCat)\n",
    "\n",
    "# calibrated LSST error model:\n",
    "# partly done in the semi-major axis part\n",
    "# here apply the overall factor:\n",
    "# lsstError_calibration['magerrscale'] \n",
    "# need to think about this step further, perhaps use a set up maps corrected for it.\n",
    "\n",
    "# selection based on non-observation in i-band:\n",
    "nan_index = np.isnan(data_degraded.data['i'])\n",
    "\n",
    "# gold cut:\n",
    "goldsel = data_degraded.data['i']<25.3\n",
    "\n",
    "# apply selections:\n",
    "data_degraded_gold = data_degraded.data[goldsel*(~nan_index)]\n",
    "# reset the index\n",
    "data_degraded_gold = data_degraded_gold.data.reset_index()\n",
    "\n",
    "# print number of objects:\n",
    "print(\"Number of objects selected in catalogue: \", len(data_degraded_gold))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c54612-1227-4627-9b61-83ca3c61e5ca",
   "metadata": {},
   "source": [
    "## Step 2: RAIL.estimation\n",
    "\n",
    "Below is an example of using `BPZ_lite` to estimate redshifts for the above sample\n",
    "- First de-redden the magnitudes\n",
    "- Point estimate of the BPZ redshifts by extracting redshift mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849644e0-6d50-4dd2-89a0-ba147af60015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPZ_lite version:\n",
    "\n",
    "band_names = ['u','g','r','i','z','y']\n",
    "band_err_names = ['u_err','g_err','r_err','i_err','z_err','y_err']\n",
    "prior_band='i'\n",
    "\n",
    "output = savedir + \"BPZ_lite_photoz.hdf5\"\n",
    "\n",
    "estimate_bpz = BPZ_lite.make_stage(name='estimate_bpz', hdf5_groupname='', \n",
    "                                   #columns_file=inroot+'test_bpz.columns',\n",
    "                                   #prior_file='CWW_HDFN_prior.pkl',\n",
    "                                   nondetect_val=np.nan, #spectra_file='SED/CWWSB4.list',\n",
    "                                   band_names=band_names,\n",
    "                                   band_err_names=band_err_names,\n",
    "                                   prior_band=prior_band,\n",
    "                                   mag_limits = dict(mag_u_lsst=27.79,\n",
    "                                                mag_g_lsst=29.04,\n",
    "                                                mag_r_lsst=29.06,\n",
    "                                                mag_i_lsst=28.62,\n",
    "                                                mag_z_lsst=27.98,\n",
    "                                                mag_y_lsst=27.05),\n",
    "                                   output=output)\n",
    "\n",
    "# note: dered is not run here because we did not add MW extinction\n",
    "test_data = convert_catalog_to_test_data(data_degraded_gold, DS, band_names, nside=nside, dered=False)\n",
    "\n",
    "bpz_estimated = estimate_bpz.estimate(test_data)\n",
    "# we can also obtain the point estimate, e.g the mode:\n",
    "zmode = pd.DataFrame(data={\"pz_point\": bpz_estimated().ancil['zmode']})\n",
    "# attach point estimate to data:\n",
    "data_degraded_gold = pd.concat([data_degraded_gold, zmode], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84a9920-297c-4525-a62d-2547bbea3603",
   "metadata": {},
   "source": [
    "## Step 3: RAIL.summarization (?) / TXPipe binning\n",
    "### Tomographic binning (dummy case):\n",
    "\n",
    "Here we include a dummy tomographic binning case, where we simply bin using the photo-z available for lens or source.\n",
    "\n",
    "In general case, we want to plug in here the result from tomo challenge/TXPipe; this will take the result from metadetection (usually with limited bands and different selection of objects); \n",
    "\n",
    "In any case, we assume here we take in a data object with a column indicating the tomographic bin it is in, but we do not specify how the binning is assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d582d20-46b8-4f5b-8bf0-5cff3b8fc036",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_degraded_gold_tomo = assign_source_bins(data_degraded_gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3636e711-5ec3-4d68-8bbe-18f8549978c3",
   "metadata": {},
   "source": [
    "### TXPipe:\n",
    "\n",
    "Using methods given by [TXPipie/lens_selector.py](https://github.com/LSSTDESC/TXPipe/blob/ad3844769f097d4e86f8ae090b1e9fbd0e99c801/txpipe/lens_selector.py) and [TXPipe/source_selector](https://github.com/LSSTDESC/TXPipe/blob/master/txpipe/source_selector.py). \n",
    "\n",
    "Notice for TXPipe, catalogue is derived from metacal or metadetect (e.g. using `riz`), and the end result is an additional column indicating which object is in which tomographic bin.\n",
    "\n",
    "Having looked at TXPipe, it seems that you can pass on photo-z for objects in the catalogue. If these are passed, then tomographic bins are split given the bin edges in pz, if not, random forest will be used with the limited bands available. \n",
    "\n",
    "MetaDetect will have different variants of the data based on which object is detected, and so tomographic bin is determined in each case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2bf2bc-62f3-442a-92c0-b3fde17c3da1",
   "metadata": {},
   "source": [
    "```\n",
    "# TXpipe related imports\n",
    "\n",
    "# Stages to run\n",
    "stages:\n",
    "    - name: FlowCreator             # Simulate a spectroscopic population\n",
    "    - name: GridSelection          # Simulate a spectroscopic sample\n",
    "    - name: TXParqetToHDF          # Convert the spec sample format\n",
    "    - name: PZPrepareEstimatorLens   # Prepare the p(z) estimator\n",
    "      classname: Inform_BPZ_lite   \n",
    "    - name: PZEstimatorLens        # Measure lens galaxy PDFs\n",
    "      classname: BPZ_lite\n",
    "      threads_per_process: 1  \n",
    "    - name: TXMeanLensSelector     # select objects for lens bins from the PDFs\n",
    "    - name: Inform_NZDirLens       # Prepare the DIR method inputs for the lens sample     \n",
    "      classname: Inform_NZDir\n",
    "    - name: PZRailSummarizeLens    # Run the DIR method on the lens sample to find n(z)\n",
    "      classname: PZRailSummarize  \n",
    "    - name: PZRailSummarizeSource  # Run the DIR method on the lens sample to find n(z)\n",
    "      classname: PZRailSummarize\n",
    "    - name: TXSourceSelectorMetadetect  # select and split objects into source bins\n",
    "    - name: Inform_NZDirSource     # Prepare the DIR method inputs for the source sample\n",
    "      classname: Inform_NZDir\n",
    "    - name: TXShearCalibration     # Calibrate and split the source sample tomographically\n",
    "    - name: TXLensCatalogSplitter  # Split the lens sample tomographically\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d711afa1-571e-4e0a-8cae-ec788fce7adb",
   "metadata": {},
   "source": [
    "## Step 4: Evaluation\n",
    "\n",
    "Here we write a simple code to evaluate the shifts and scatter of the photo-z bins for different depth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f164de1-a00c-420d-8b5f-0c61160812dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now for each quantile, find the data and sample the redshifts distirbution:\n",
    "\n",
    "# split the systematic maps into 20 quantiles\n",
    "nquantiles=20\n",
    "# define the zgrid for nz\n",
    "zgrid = np.linspace(0,3,101)\n",
    "# number of bootstrap samples to use\n",
    "nbootstrap=1000\n",
    "# which redshift in the data set to use for nz\n",
    "z_col = 'redshift' # true redshift, but can change here to pz_mode or something\n",
    "# number of tomographic bins used \n",
    "npzbins=data_degraded_gold_tomo[\"tomo\"].max()\n",
    "\n",
    "# load the specific systematic map & mask to check correlation\n",
    "mapin = hp.read_map(sys_to_check)\n",
    "mask = hp.read_map(maskdir)\n",
    "# quantile contains pixel indices, and meanv is the mean value of the systematic maps in each quantile\n",
    "quantile, meanv = split_sys_map_quantiles(mapin, mask, nquantiles=nquantiles)\n",
    "\n",
    "# compute simple summary statistic\n",
    "nzstat_summary_split={}\n",
    "\n",
    "for jj in range(npzbins):\n",
    "    nzstat_summary_split[\"tomo-%d\"%(jj+1)]={}\n",
    "    \n",
    "    ind = data_degraded_gold_tomo[\"tomo\"] == (jj+1)\n",
    "\n",
    "    for ii in range(nquantiles):\n",
    "        ind *= np.in1d(data_degraded_gold_tomo[\"pixels\"], quantile[ii])\n",
    "        usecat = data_degraded_gold_tomo.loc[ind, :]\n",
    "        # now for each tomographic bin, return redshift distribution:\n",
    "        nzstat_summary_split[\"tomo-%d\"%(jj+1)][ii] = compute_nzstats(usecat, z_col, \n",
    "                                                                     zgrid=zgrid, nbootstrap=nbootstrap)\n",
    "\n",
    "    # compute the tot nz, meanz, sigmaz:\n",
    "    nzstat_summary_tot[\"tomo-%d\"%(jj+1)] = compute_nzstats(data_degraded_gold_tomo, z_col, \n",
    "                                                           zgrid=zgrid, nbootstrap=nbootstrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbce9e68-4cee-4652-9af4-d66083a74359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file:\n",
    "outroot = savedir + \"test-pz-with-i-band-coadd-Y1.yml\"\n",
    "write_evaluation_results(outroot, meanv, nzstat_summary_split, nzstat_summary_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a88d8a-ba42-44b6-8141-e8f28d466072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show results in a plot:\n",
    "fig,axarr=plt.subplots(3,npzbins,figsize=[15,10],gridspec_kw={'height_ratios': [3, 1, 1]})\n",
    "\n",
    "## Top row: n(z) for each tomographic bin for each depth group\n",
    "## Middle row: change in meanz as a function of depth with bootstrap errors\n",
    "## Bottom row: change in sigmaz as a function of depth with boostrap errors\n",
    "\n",
    "for ii in range(npzbins):\n",
    "    \n",
    "    # top row\n",
    "    plt.sca(axarr[0,ii])\n",
    "    for q in range(nquantiles):\n",
    "        colorlab = q/(nquantiles*1.2)\n",
    "        nz = stat_summary_split[\"tomo-%d\"%(ii+1)][q][0]\n",
    "        plt.plot(nz[:,0], nz[:,1]/np.sum(nz[:,1])/(nz[1,0]-nz[0,0]), \n",
    "                color=cmap(colorlab))\n",
    "    plt.text(0.6, 3.5, \"tomo-%d\"%(ii+1))\n",
    "    plt.yticks([])\n",
    "    plt.xlabel(\"$z$\")\n",
    "    \n",
    "    # middle row\n",
    "    plt.sca(axarr[1, ii])\n",
    "    for q in range(nquantiles):\n",
    "        colorlab = q/(nquantiles*1.2)\n",
    "        \n",
    "        meanz = stat_summary_split[\"tomo-%d\"%(ii+1)][q][1]\n",
    "        meanztot = stat_summary_tot[\"tomo-%d\"%(ii+1)][q][1]\n",
    "        \n",
    "        plt.errorbar(meanv[q], meanz[0], yerr=meanz[1],fmt='o',\n",
    "                    color=cmap(colorlab))\n",
    "    #dz = 0.005*(1+meanztot[0])\n",
    "    plt.plot(meanv, np.ones(len(meanv))*meanztot[0], 'k-', alpha=0.5)\n",
    "    #plt.fill_between([meanv[0], meanv[-1]], [-dz, -dz], \n",
    "                    #[dz, dz],color='k',alpha=0.2)\n",
    "    if ii==0:\n",
    "        plt.ylabel(\"$\\\\langle z\\\\rangle$\")\n",
    "    if ii>0:\n",
    "        plt.yticks([])\n",
    "    #plt.xlabel(sys)\n",
    "    #plt.ylim([-0.015,0.015])\n",
    "    #plt.xlim([24.6, 25.7])\n",
    "    \n",
    "    # bottom row\n",
    "    plt.sca(axarr[2, ii])\n",
    "    for q in range(nquantiles):\n",
    "        colorlab = q/(nquantiles*1.2)\n",
    "        \n",
    "        sigmaz = stat_summary_split[\"tomo-%d\"%(ii+1)][q][2]\n",
    "        sigmaztot = stat_summary_tot[\"tomo-%d\"%(ii+1)][q][2]\n",
    "        \n",
    "        plt.errorbar(meanv[q], sigmaz[0], yerr=sigmaz[1],fmt='o',\n",
    "                    color=cmap(colorlab))\n",
    "    #dz = 0.005\n",
    "    plt.plot(meanv, np.ones(len(meanv))*sigmaztot[0], 'k-', alpha=0.5)\n",
    "    #plt.fill_between([meanv[0], meanv[-1]], [-dz, -dz], \n",
    "                    #[dz, dz],color='k',alpha=0.2)\n",
    "    if ii==0:\n",
    "        plt.ylabel(\"$\\\\sigma_z$\")\n",
    "    if ii>0:\n",
    "        plt.yticks([])\n",
    "    plt.xlabel(sys)\n",
    "    #plt.ylim([-0.015,0.015])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.saveifg(savedir + 'fig.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab0f305-914f-4cdd-8c0f-9d90fbeec36f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condarail",
   "language": "python",
   "name": "condarail"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
